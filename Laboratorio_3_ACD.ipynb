{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 1: Preparación del dataset"
      ],
      "metadata": {
        "id": "35YIc9rhAVlx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8OSuMXUv_yw2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generación del dataset\n",
        "d = 100  # número de columnas\n",
        "n = 1000  # número de observaciones\n",
        "\n",
        "# Matriz A con entradas aleatorias de una distribución normal\n",
        "A = np.random.normal(0, 1, size=(n, d))\n",
        "\n",
        "# Vector verdadero x* (coeficientes)\n",
        "x_true = np.random.normal(0, 1, size=(d, 1))\n",
        "\n",
        "# Vector b con ruido añadido\n",
        "b = A.dot(x_true) + np.random.normal(0, 0.5, size=(n, 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 2: Solución Cerrada"
      ],
      "metadata": {
        "id": "OtwUXN4wBOzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solución cerrada\n",
        "def closed_form_solution(A, b):\n",
        "    return np.linalg.inv(A.T.dot(A)).dot(A.T).dot(b)\n",
        "\n",
        "x_closed_form = closed_form_solution(A, b)\n"
      ],
      "metadata": {
        "id": "xDigR7hWBOZm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 3: Gradiente Descendente (GD)"
      ],
      "metadata": {
        "id": "Qr9eFtccCIHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(A, b, learning_rate, max_iter=1000, tolerance=1e-6):\n",
        "    n, d = A.shape\n",
        "    x = np.zeros((d, 1))  # Inicialización en ceros\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        # Calculamos el gradiente\n",
        "        grad = 2 * A.T.dot(A.dot(x) - b)\n",
        "        # Actualizamos el vector de coeficientes\n",
        "        x_new = x - learning_rate * grad\n",
        "        # Calculamos el costo\n",
        "        cost = np.sum((A.dot(x_new) - b) ** 2)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        # Criterio de convergencia\n",
        "        if np.linalg.norm(x_new - x) < tolerance:\n",
        "            break\n",
        "        x = x_new\n",
        "\n",
        "    return x, cost_history\n",
        "\n",
        "# Ejecutamos el GD con diferentes tasas de aprendizaje\n",
        "learning_rates = [0.00005, 0.0005, 0.0007]\n",
        "gd_results = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    x_gd, cost_history = gradient_descent(A, b, learning_rate=lr)\n",
        "    gd_results[lr] = cost_history\n"
      ],
      "metadata": {
        "id": "h_7NZN05CGeE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 4: Descenso de Gradiente Estocástico (SGD)"
      ],
      "metadata": {
        "id": "PBV6EyiXCV7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(A, b, learning_rate, max_iter=1000):\n",
        "    n, d = A.shape\n",
        "    x = np.zeros((d, 1))  # Inicialización en ceros\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        for j in range(n):\n",
        "            # Seleccionamos una muestra al azar\n",
        "            random_index = np.random.randint(n)\n",
        "            a_j = A[random_index, :].reshape(1, d)\n",
        "            b_j = b[random_index].reshape(1, 1)\n",
        "            # Gradiente para una muestra\n",
        "            grad = 2 * a_j.T.dot(a_j.dot(x) - b_j)\n",
        "            # Actualizamos el vector de coeficientes\n",
        "            x = x - learning_rate * grad\n",
        "        # Costo después de cada iteración\n",
        "        cost = np.sum((A.dot(x) - b) ** 2)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return x, cost_history\n",
        "\n",
        "# Ejecutamos el SGD con diferentes tasas de aprendizaje\n",
        "learning_rates_sgd = [0.0005, 0.005, 0.01]\n",
        "sgd_results = {}\n",
        "\n",
        "for lr in learning_rates_sgd:\n",
        "    x_sgd, cost_history_sgd = stochastic_gradient_descent(A, b, learning_rate=lr)\n",
        "    sgd_results[lr] = cost_history_sgd\n"
      ],
      "metadata": {
        "id": "3shNx40KCY4o"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 5: Descenso de Gradiente en Mini-Batch (MBGD)"
      ],
      "metadata": {
        "id": "Z5ue88o9DBzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_batch_gradient_descent(A, b, learning_rate, batch_size, max_iter=1000):\n",
        "    n, d = A.shape\n",
        "    x = np.zeros((d, 1))  # Inicialización en ceros\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        # Permutar las filas de la matriz A\n",
        "        permutation = np.random.permutation(n)\n",
        "        A_shuffled = A[permutation]\n",
        "        b_shuffled = b[permutation]\n",
        "\n",
        "        # Actualizamos en lotes\n",
        "        for j in range(0, n, batch_size):\n",
        "            A_batch = A_shuffled[j:j+batch_size]\n",
        "            b_batch = b_shuffled[j:j+batch_size]\n",
        "            grad = 2 * A_batch.T.dot(A_batch.dot(x) - b_batch)\n",
        "            x = x - learning_rate * grad\n",
        "\n",
        "        # Costo después de cada iteración\n",
        "        cost = np.sum((A.dot(x) - b) ** 2)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return x, cost_history\n",
        "\n",
        "# Ejecutamos el MBGD con diferentes tamaños de batch y tasas de aprendizaje\n",
        "batch_sizes = [25, 50, 100]\n",
        "learning_rates_mbgd = [0.0005, 0.005, 0.01]\n",
        "mbgd_results = {}\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    for lr in learning_rates_mbgd:\n",
        "        x_mbgd, cost_history_mbgd = mini_batch_gradient_descent(A, b, learning_rate=lr, batch_size=batch_size)\n",
        "        mbgd_results[(batch_size, lr)] = cost_history_mbgd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2igvdnBIDEYI",
        "outputId": "7ac7b3c8-4cf2-407e-b201-0f23fd29ff46"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "<ipython-input-5-7df4545d657c>:20: RuntimeWarning: overflow encountered in square\n",
            "  cost = np.sum((A.dot(x) - b) ** 2)\n",
            "<ipython-input-5-7df4545d657c>:16: RuntimeWarning: overflow encountered in multiply\n",
            "  grad = 2 * A_batch.T.dot(A_batch.dot(x) - b_batch)\n",
            "<ipython-input-5-7df4545d657c>:17: RuntimeWarning: invalid value encountered in subtract\n",
            "  x = x - learning_rate * grad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 6: Método de Newton"
      ],
      "metadata": {
        "id": "biQzK8jqDM52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rosenbrock(x1, x2):\n",
        "    return 100 * (x2 - x1**2)**2 + (1 - x1)**2\n",
        "\n",
        "def gradient_rosenbrock(x1, x2):\n",
        "    dfdx1 = -400 * (x2 - x1**2) * x1 - 2 * (1 - x1)\n",
        "    dfdx2 = 200 * (x2 - x1**2)\n",
        "    return np.array([dfdx1, dfdx2])\n",
        "\n",
        "def hessian_rosenbrock(x1, x2):\n",
        "    d2fdx1x1 = 1200 * x1**2 - 400 * x2 + 2\n",
        "    d2fdx1x2 = -400 * x1\n",
        "    d2fdx2x2 = 200\n",
        "    return np.array([[d2fdx1x1, d2fdx1x2], [d2fdx1x2, d2fdx2x2]])\n",
        "\n",
        "def newton_method_rosenbrock(x_init, max_iter=1000, tol=1e-8):\n",
        "    x = np.array(x_init)\n",
        "    for i in range(max_iter):\n",
        "        grad = gradient_rosenbrock(x[0], x[1])\n",
        "        hess = hessian_rosenbrock(x[0], x[1])\n",
        "        delta_x = np.linalg.inv(hess).dot(grad)\n",
        "        x_new = x - delta_x\n",
        "\n",
        "        if np.linalg.norm(grad) < tol:\n",
        "            break\n",
        "\n",
        "        x = x_new\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "x_init = [0, 0]\n",
        "x_newton = newton_method_rosenbrock(x_init)\n"
      ],
      "metadata": {
        "id": "qg9gbNRVDOrj"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}