{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 1: Preparación del dataset"
      ],
      "metadata": {
        "id": "35YIc9rhAVlx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8OSuMXUv_yw2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generación del dataset\n",
        "d = 100  # número de columnas\n",
        "n = 1000  # número de observaciones\n",
        "\n",
        "# Matriz A con entradas aleatorias de una distribución normal\n",
        "A = np.random.normal(0, 1, size=(n, d))\n",
        "\n",
        "# Vector verdadero x* (coeficientes)\n",
        "x_true = np.random.normal(0, 1, size=(d, 1))\n",
        "\n",
        "# Vector b con ruido añadido\n",
        "b = A.dot(x_true) + np.random.normal(0, 0.5, size=(n, 1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 2: Solución Cerrada"
      ],
      "metadata": {
        "id": "OtwUXN4wBOzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solución cerrada\n",
        "def closed_form_solution(A, b):\n",
        "    return np.linalg.inv(A.T.dot(A)).dot(A.T).dot(b)\n",
        "\n",
        "x_closed_form = closed_form_solution(A, b)\n"
      ],
      "metadata": {
        "id": "xDigR7hWBOZm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 3: Gradiente Descendente (GD)"
      ],
      "metadata": {
        "id": "Qr9eFtccCIHr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(A, b, learning_rate, max_iter=1000, tolerance=1e-6):\n",
        "    n, d = A.shape\n",
        "    x = np.zeros((d, 1))  # Inicialización en ceros\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        # Calculamos el gradiente\n",
        "        grad = 2 * A.T.dot(A.dot(x) - b)\n",
        "        # Actualizamos el vector de coeficientes\n",
        "        x_new = x - learning_rate * grad\n",
        "        # Calculamos el costo\n",
        "        cost = np.sum((A.dot(x_new) - b) ** 2)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "        # Criterio de convergencia\n",
        "        if np.linalg.norm(x_new - x) < tolerance:\n",
        "            break\n",
        "        x = x_new\n",
        "\n",
        "    return x, cost_history\n",
        "\n",
        "# Ejecutamos el GD con diferentes tasas de aprendizaje\n",
        "learning_rates = [0.00005, 0.0005, 0.0007]\n",
        "gd_results = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    x_gd, cost_history = gradient_descent(A, b, learning_rate=lr)\n",
        "    gd_results[lr] = cost_history\n"
      ],
      "metadata": {
        "id": "h_7NZN05CGeE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 4: Descenso de Gradiente Estocástico (SGD)"
      ],
      "metadata": {
        "id": "PBV6EyiXCV7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(A, b, learning_rate, max_iter=1000):\n",
        "    n, d = A.shape\n",
        "    x = np.zeros((d, 1))  # Inicialización en ceros\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        for j in range(n):\n",
        "            # Seleccionamos una muestra al azar\n",
        "            random_index = np.random.randint(n)\n",
        "            a_j = A[random_index, :].reshape(1, d)\n",
        "            b_j = b[random_index].reshape(1, 1)\n",
        "            # Gradiente para una muestra\n",
        "            grad = 2 * a_j.T.dot(a_j.dot(x) - b_j)\n",
        "            # Actualizamos el vector de coeficientes\n",
        "            x = x - learning_rate * grad\n",
        "        # Costo después de cada iteración\n",
        "        cost = np.sum((A.dot(x) - b) ** 2)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return x, cost_history\n",
        "\n",
        "# Ejecutamos el SGD con diferentes tasas de aprendizaje\n",
        "learning_rates_sgd = [0.0005, 0.005, 0.01]\n",
        "sgd_results = {}\n",
        "\n",
        "for lr in learning_rates_sgd:\n",
        "    x_sgd, cost_history_sgd = stochastic_gradient_descent(A, b, learning_rate=lr)\n",
        "    sgd_results[lr] = cost_history_sgd\n"
      ],
      "metadata": {
        "id": "3shNx40KCY4o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61876665-c3b8-412d-b00f-d4a0c61986ba"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
            "<ipython-input-4-076e9f2d771f>:17: RuntimeWarning: overflow encountered in square\n",
            "  cost = np.sum((A.dot(x) - b) ** 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 5: Descenso de Gradiente en Mini-Batch (MBGD)"
      ],
      "metadata": {
        "id": "Z5ue88o9DBzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_batch_gradient_descent(A, b, learning_rate, batch_size, max_iter=1000):\n",
        "    n, d = A.shape\n",
        "    x = np.zeros((d, 1))  # Inicialización en ceros\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        # Permutar las filas de la matriz A\n",
        "        permutation = np.random.permutation(n)\n",
        "        A_shuffled = A[permutation]\n",
        "        b_shuffled = b[permutation]\n",
        "\n",
        "        # Actualizamos en lotes\n",
        "        for j in range(0, n, batch_size):\n",
        "            A_batch = A_shuffled[j:j+batch_size]\n",
        "            b_batch = b_shuffled[j:j+batch_size]\n",
        "            grad = 2 * A_batch.T.dot(A_batch.dot(x) - b_batch)\n",
        "            x = x - learning_rate * grad\n",
        "\n",
        "        # Costo después de cada iteración\n",
        "        cost = np.sum((A.dot(x) - b) ** 2)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return x, cost_history\n",
        "\n",
        "# Ejecutamos el MBGD con diferentes tamaños de batch y tasas de aprendizaje\n",
        "batch_sizes = [25, 50, 100]\n",
        "learning_rates_mbgd = [0.0005, 0.005, 0.01]\n",
        "mbgd_results = {}\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    for lr in learning_rates_mbgd:\n",
        "        x_mbgd, cost_history_mbgd = mini_batch_gradient_descent(A, b, learning_rate=lr, batch_size=batch_size)\n",
        "        mbgd_results[(batch_size, lr)] = cost_history_mbgd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2igvdnBIDEYI",
        "outputId": "d657e696-cff8-4798-a500-1d539fda7010"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-44640032df9d>:20: RuntimeWarning: overflow encountered in square\n",
            "  cost = np.sum((A.dot(x) - b) ** 2)\n",
            "<ipython-input-5-44640032df9d>:16: RuntimeWarning: overflow encountered in multiply\n",
            "  grad = 2 * A_batch.T.dot(A_batch.dot(x) - b_batch)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 6: Método de Newton"
      ],
      "metadata": {
        "id": "biQzK8jqDM52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función de Rosenbrock\n",
        "def rosenbrock(x):\n",
        "    x1, x2 = x[0], x[1]\n",
        "    return 100 * (x2 - x1**2)**2 + (1 - x1)**2\n",
        "\n",
        "# Gradiente de la función de Rosenbrock\n",
        "def grad_rosenbrock(x):\n",
        "    x1, x2 = x[0], x[1]\n",
        "    grad_x1 = -400 * x1 * (x2 - x1**2) - 2 * (1 - x1)\n",
        "    grad_x2 = 200 * (x2 - x1**2)\n",
        "    return np.array([grad_x1, grad_x2])\n",
        "\n",
        "# Hessiana de la función de Rosenbrock (solo para el método de Newton)\n",
        "def hessian_rosenbrock(x):\n",
        "    x1, x2 = x[0], x[1]\n",
        "    hessian = np.array([\n",
        "        [-400 * (x2 - 3 * x1**2) + 2, -400 * x1],\n",
        "        [-400 * x1, 200]\n",
        "    ])\n",
        "    return hessian\n"
      ],
      "metadata": {
        "id": "OJUsEuJoapzk"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Backtracking Line Search\n",
        "def backtracking_line_search(f, grad, x, p, alpha=0.3, beta=0.8):\n",
        "    t = 1  # tamaño de paso inicial\n",
        "    while f(x + t * p) > f(x) + alpha * t * np.dot(grad(x), p):\n",
        "        t *= beta\n",
        "    return t\n"
      ],
      "metadata": {
        "id": "ck0IPiw6bQ8W"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradiente Descendente con Backtracking Line Search\n",
        "def gradient_descent_backtracking(x_init, tol=1e-8, max_iter=1000):\n",
        "    x = np.array(x_init)\n",
        "    history = [x]\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        grad = grad_rosenbrock(x)\n",
        "        p = -grad  # dirección de descenso (negativo del gradiente)\n",
        "\n",
        "        # Aplicamos Backtracking Line Search para obtener el tamaño de paso t\n",
        "        t = backtracking_line_search(rosenbrock, grad_rosenbrock, x, p)\n",
        "\n",
        "        # Actualizamos x\n",
        "        x_new = x + t * p\n",
        "        history.append(x_new)\n",
        "\n",
        "        # Verificamos si hemos llegado a una convergencia suficiente\n",
        "        if np.linalg.norm(grad) < tol:\n",
        "            break\n",
        "\n",
        "        x = x_new\n",
        "\n",
        "    return np.array(history)\n"
      ],
      "metadata": {
        "id": "O8__A029bVyx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Método de Newton con Backtracking Line Search\n",
        "def newton_method_backtracking(x_init, tol=1e-8, max_iter=1000):\n",
        "    x = np.array(x_init)\n",
        "    history = [x]\n",
        "\n",
        "    for i in range(max_iter):\n",
        "        grad = grad_rosenbrock(x)\n",
        "        hess = hessian_rosenbrock(x)\n",
        "        p = -np.linalg.inv(hess).dot(grad)  # dirección de descenso (Newton)\n",
        "\n",
        "        # Aplicamos Backtracking Line Search para obtener el tamaño de paso t\n",
        "        t = backtracking_line_search(rosenbrock, grad_rosenbrock, x, p)\n",
        "\n",
        "        # Actualizamos x\n",
        "        x_new = x + t * p\n",
        "        history.append(x_new)\n",
        "\n",
        "        # Verificamos si hemos llegado a una convergencia suficiente\n",
        "        if np.linalg.norm(grad) < tol:\n",
        "            break\n",
        "\n",
        "        x = x_new\n",
        "\n",
        "    return np.array(history)\n"
      ],
      "metadata": {
        "id": "Owwh-d6wbdbQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Puntos iniciales sugeridos\n",
        "initial_points = [(0, 0), (0.6, 0.6), (-0.5, 1), (-1.2, 1)]\n",
        "\n",
        "# Ejecutar Gradiente Descendente y Método de Newton desde diferentes puntos iniciales\n",
        "for x_init in initial_points:\n",
        "    print(f\"\\nStarting Point: {x_init}\")\n",
        "\n",
        "    # Gradiente Descendente\n",
        "    gd_history = gradient_descent_backtracking(x_init)\n",
        "    print(f\"GD Final Point: {gd_history[-1]} after {len(gd_history)} iterations\")\n",
        "\n",
        "    # Método de Newton\n",
        "    newton_history = newton_method_backtracking(x_init)\n",
        "    print(f\"Newton Final Point: {newton_history[-1]} after {len(newton_history)} iterations\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhV_I9n9bnfT",
        "outputId": "afef9b07-cb17-4db3-c5b9-ecc84d576f32"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting Point: (0, 0)\n",
            "GD Final Point: [0.90386868 0.81680512] after 1001 iterations\n",
            "Newton Final Point: [1. 1.] after 16 iterations\n",
            "\n",
            "Starting Point: (0.6, 0.6)\n",
            "GD Final Point: [0.92526139 0.85602713] after 1001 iterations\n",
            "Newton Final Point: [1. 1.] after 12 iterations\n",
            "\n",
            "Starting Point: (-0.5, 1)\n",
            "GD Final Point: [0.91903753 0.8440474 ] after 1001 iterations\n",
            "Newton Final Point: [1. 1.] after 20 iterations\n",
            "\n",
            "Starting Point: (-1.2, 1)\n",
            "GD Final Point: [0.93171918 0.86765623] after 1001 iterations\n",
            "Newton Final Point: [1. 1.] after 23 iterations\n"
          ]
        }
      ]
    }
  ]
}